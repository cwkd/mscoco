{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/laura/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from model import Encoder, Decoder\n",
    "from dataloader2 import Vocabulary, MSCOCODataset, one_hot_encode, un_one_hot_encode\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip\n",
      "Requirement already satisfied: six in /home/laura/anaconda3/envs/my_env/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Collecting singledispatch (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/laura/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
      "Successfully built nltk\n",
      "Installing collected packages: singledispatch, nltk\n",
      "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epochs=10, encoder= None, model=None, train_loader=None, val_loader=None, \n",
    "          optimizer=None, loss_fn=None, vocab_size=None, verbose=False, print_every=2000):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    batch_size = train_loader.batch_size\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    loss = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "#         model.train()\n",
    "#         for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#             gt = target.copy()\n",
    "#             target = torch.cat(target[:-1]).unsqueeze(0)\n",
    "#             gt = torch.cat(gt[1:]).unsqueeze(0)\n",
    "#             data, target, gt = data.to(device), target.to(device), gt.to(device)\n",
    "#             features = encoder(data).unsqueeze(0)\n",
    "#             features = features.repeat([1, gt.shape[1], 1])\n",
    "#             #print(features.shape)\n",
    "#             output, (hidden, cs) = model(features, target)\n",
    "#             output = softmax(output)\n",
    "#             #optimizer.zero_grad()\n",
    "#             #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "#             #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "#             #print(data.shape, target.shape)\n",
    "#             loss = 0\n",
    "#             for b in range(batch_size):\n",
    "#                 for e in range(len(gt[b])):\n",
    "#                     l = loss_fn(output[b,e].unsqueeze(0), gt[b,e].long().unsqueeze(0))\n",
    "#                     loss += l\n",
    "#             loss/batch_size\n",
    "#             #pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "#             #print(pred.shape, target.shape)\n",
    "#             #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#             if verbose and batch_idx*batch_size % print_every < batch_size:\n",
    "#                 print('Batch Number: {}\\t\\t[{}/{}\\t({:.3f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                         batch_idx + 1, (batch_idx +1)* train_loader.batch_size, len(train_loader),\n",
    "#                         100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             #if batch_idx*batch_size % 5000 < batch_size and batch_idx!=0:\n",
    "#             #    sample(model, temp)\n",
    "            \n",
    "#         train_losses.append(loss.item())\n",
    "        \n",
    "#         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#             epoch + 1, (batch_idx+1) * train_loader.batch_size, len(train_loader),\n",
    "#             100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in val_loader:\n",
    "                gt = target.copy()\n",
    "                target = torch.cat(target[:-1]).unsqueeze(0)\n",
    "                gt = torch.cat(gt[1:]).unsqueeze(0)\n",
    "                data, target, gt = data.to(device), target.to(device), gt.to(device)\n",
    "                features = encoder(data).unsqueeze(0)\n",
    "                features = features.repeat([1, gt.shape[1], 1])\n",
    "                output, (hidden, cs) = model(features, target)\n",
    "                #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "                #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "                #print(features.shape)\n",
    "                output, (hidden, cs) = model(features, target)\n",
    "                output = softmax(output)\n",
    "                loss = 0\n",
    "                for b in range(batch_size):\n",
    "                    for e in range(len(gt[b])):\n",
    "                        l = loss_fn(output[b,e].unsqueeze(0), gt[b,e].long().unsqueeze(0))\n",
    "                        loss += l\n",
    "                batch_loss = loss.item()\n",
    "                val_loss += batch_loss\n",
    "                pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "                #print(pred, target)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                if verbose and batch_idx*batch_size % print_every < batch_size:\n",
    "                    print('Batch Number: {}\\t\\t[{}/{}\\t({:.3f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            batch_idx + 1, (batch_idx +1)* val_loader.batch_size, len(val_loader),\n",
    "                            100. * batch_idx / (len(val_loader)/val_loader.batch_size), loss.item()))\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss < best_loss[0]:\n",
    "               print('\\nSaving model parameters in epoch {}'.format(epoch+1))\n",
    "               torch.save(model.state_dict(), \"./temp_model.pt\")\n",
    "               best_loss[0] = val_loss\n",
    "               save_flag = True\n",
    "\n",
    "            val_acc = 100. * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            print('Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                val_loss, correct, total,\n",
    "                val_acc))\n",
    "            \n",
    "        #sample(model, temp)\n",
    "        \n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_epochs=20, batch_size=1, lr=0.01):\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    vocab_size = len(vocab)\n",
    "    #print(vocab.word2idx['<end>'])\n",
    "    \n",
    "    print('Vocab Size: {}'.format(vocab_size))\n",
    "    transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),])\n",
    "    \n",
    "    \n",
    "    trainset = MSCOCODataset(root = '../train_images/train2014/',\n",
    "                            annFile = '../annotations_trainval2014/annotations/captions_train2014.json',\n",
    "                            transform=transform,\n",
    "                             vocab=vocab)\n",
    "    print('Number of training samples: ', len(trainset))\n",
    "    \n",
    "    valset = MSCOCODataset(root = '../val_images/val2014',\n",
    "                            annFile = '../annotations_trainval2014/annotations/captions_val2014.json',\n",
    "                            transform=transform, vocab=vocab)\n",
    "\n",
    "    \n",
    "    print('Number of validation samples: ', len(valset))\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    encoder = Encoder().to(device)\n",
    "    decoder = Decoder(feat_size=4096, embed_size=1000, \n",
    "                      vocab_size=vocab_size, hidden_size=300, \n",
    "                      num_layers=3).to(device)\n",
    "    \n",
    "    train_loader = data.DataLoader(trainset, batch_size, shuffle=False)\n",
    "    val_loader = data.DataLoader(valset, batch_size, shuffle=False)\n",
    "    optimizer = optim.RMSprop(decoder.parameters(), lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    train_losses, val_losses, val_accs = train(max_epochs, encoder, decoder, train_loader, val_loader, \n",
    "                                               optimizer, loss_fn, vocab_size, verbose=True)\n",
    "    \n",
    "    best_model_wts = decoder.state_dict()\n",
    "    torch.save(best_model_wts,'./model.pt')\n",
    "\n",
    "    epochs = np.arange(1, max_epochs+1)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(epochs, train_losses, epochs, val_losses)\n",
    "    plt.legend(['Training', 'Validation'],loc='upper right')\n",
    "    plt.title('Training and Validation Loss per epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(epochs, val_accs)\n",
    "    plt.legend(['Validation'],loc='upper right')\n",
    "    plt.title('Validation Accuracy per epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    train_losses = np.asarray(train_losses)\n",
    "    val_losses = np.asarray(val_losses)\n",
    "    val_accs = np.asarray(val_accs)\n",
    "    np.save('./train_losses.npy',train_losses)\n",
    "    np.save('./val_losses.npy',train_losses)\n",
    "    np.save('./val_accs.npy',train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6071\n",
      "loading annotations into memory...\n",
      "Done (t=1.40s)\n",
      "Done (t=0.54s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training samples:  414113\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation samples:  202654\n",
      "Batch Number: 1\t\t[1/414113\t(0.000%)]\tLoss: 19.767372\n",
      "Train Epoch: 1 [1/414113 (0%)]\tLoss: 19.767372\n",
      "Val set: Average loss: 0.0000, Accuracy: 0/12 (0.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Daniel\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation samples:  202654\n",
      "Batch Number: 1\t\t[1/414113\t(0.000%)]\tLoss: 19.792065\n",
      "Batch Number: 2001\t\t[2001/414113\t(0.483%)]\tLoss: 90.532104\n",
      "Batch Number: 4001\t\t[4001/414113\t(0.966%)]\tLoss: 56.386246\n",
      "Batch Number: 6001\t\t[6001/414113\t(1.449%)]\tLoss: 65.215408\n",
      "Batch Number: 8001\t\t[8001/414113\t(1.932%)]\tLoss: 35.804214\n",
      "Batch Number: 10001\t\t[10001/414113\t(2.415%)]\tLoss: 28.710205\n",
      "Batch Number: 12001\t\t[12001/414113\t(2.898%)]\tLoss: 33.188324\n",
      "Batch Number: 14001\t\t[14001/414113\t(3.381%)]\tLoss: 36.809723\n",
      "Batch Number: 16001\t\t[16001/414113\t(3.864%)]\tLoss: 229.664856\n",
      "Batch Number: 18001\t\t[18001/414113\t(4.347%)]\tLoss: 634.420898\n",
      "Batch Number: 20001\t\t[20001/414113\t(4.830%)]\tLoss: 68.034607\n",
      "Batch Number: 22001\t\t[22001/414113\t(5.313%)]\tLoss: 164.721100\n",
      "Batch Number: 24001\t\t[24001/414113\t(5.796%)]\tLoss: 5538.369141\n",
      "Batch Number: 26001\t\t[26001/414113\t(6.278%)]\tLoss: 562.659180\n",
      "Batch Number: 28001\t\t[28001/414113\t(6.761%)]\tLoss: 31.847168\n",
      "Batch Number: 30001\t\t[30001/414113\t(7.244%)]\tLoss: 619.257568\n",
      "Batch Number: 32001\t\t[32001/414113\t(7.727%)]\tLoss: 27.188110\n",
      "Batch Number: 34001\t\t[34001/414113\t(8.210%)]\tLoss: 685.039795\n",
      "Batch Number: 36001\t\t[36001/414113\t(8.693%)]\tLoss: 768.908752\n",
      "Batch Number: 38001\t\t[38001/414113\t(9.176%)]\tLoss: 79.536865\n",
      "Batch Number: 40001\t\t[40001/414113\t(9.659%)]\tLoss: 193.152832\n",
      "Batch Number: 42001\t\t[42001/414113\t(10.142%)]\tLoss: 47.638306\n",
      "Batch Number: 44001\t\t[44001/414113\t(10.625%)]\tLoss: 458.295807\n",
      "Batch Number: 46001\t\t[46001/414113\t(11.108%)]\tLoss: 185.851501\n",
      "Batch Number: 48001\t\t[48001/414113\t(11.591%)]\tLoss: 47.256042\n",
      "Batch Number: 50001\t\t[50001/414113\t(12.074%)]\tLoss: 5252.107422\n",
      "Batch Number: 52001\t\t[52001/414113\t(12.557%)]\tLoss: 43.711548\n",
      "Batch Number: 54001\t\t[54001/414113\t(13.040%)]\tLoss: 230.732544\n",
      "Batch Number: 56001\t\t[56001/414113\t(13.523%)]\tLoss: 154.683472\n",
      "Batch Number: 58001\t\t[58001/414113\t(14.006%)]\tLoss: 87.695312\n",
      "Batch Number: 60001\t\t[60001/414113\t(14.489%)]\tLoss: 91.233421\n",
      "Batch Number: 62001\t\t[62001/414113\t(14.972%)]\tLoss: 28.650635\n",
      "Batch Number: 64001\t\t[64001/414113\t(15.455%)]\tLoss: 1658.407227\n",
      "Batch Number: 66001\t\t[66001/414113\t(15.938%)]\tLoss: 271.454071\n",
      "Batch Number: 68001\t\t[68001/414113\t(16.421%)]\tLoss: 75.597717\n",
      "Batch Number: 70001\t\t[70001/414113\t(16.904%)]\tLoss: 346.531616\n",
      "Batch Number: 72001\t\t[72001/414113\t(17.387%)]\tLoss: 218.556396\n",
      "Batch Number: 74001\t\t[74001/414113\t(17.870%)]\tLoss: 146.088989\n",
      "Batch Number: 76001\t\t[76001/414113\t(18.352%)]\tLoss: 393.131348\n",
      "Batch Number: 78001\t\t[78001/414113\t(18.835%)]\tLoss: 22244.251953\n",
      "Batch Number: 80001\t\t[80001/414113\t(19.318%)]\tLoss: 283.705322\n",
      "Batch Number: 82001\t\t[82001/414113\t(19.801%)]\tLoss: 407.070007\n",
      "Batch Number: 84001\t\t[84001/414113\t(20.284%)]\tLoss: 35.436279\n",
      "Batch Number: 86001\t\t[86001/414113\t(20.767%)]\tLoss: 575.747681\n",
      "Batch Number: 88001\t\t[88001/414113\t(21.250%)]\tLoss: 472.192871\n",
      "Batch Number: 90001\t\t[90001/414113\t(21.733%)]\tLoss: 700.976562\n",
      "Batch Number: 92001\t\t[92001/414113\t(22.216%)]\tLoss: 425.580078\n",
      "Batch Number: 94001\t\t[94001/414113\t(22.699%)]\tLoss: 531.634766\n",
      "Batch Number: 96001\t\t[96001/414113\t(23.182%)]\tLoss: 56.107178\n",
      "Batch Number: 98001\t\t[98001/414113\t(23.665%)]\tLoss: 384.411621\n",
      "Batch Number: 100001\t\t[100001/414113\t(24.148%)]\tLoss: 365.417969\n",
      "Batch Number: 102001\t\t[102001/414113\t(24.631%)]\tLoss: 19.492188\n",
      "Batch Number: 104001\t\t[104001/414113\t(25.114%)]\tLoss: 29.389648\n",
      "Batch Number: 106001\t\t[106001/414113\t(25.597%)]\tLoss: 27.100586\n",
      "Batch Number: 108001\t\t[108001/414113\t(26.080%)]\tLoss: 61.188309\n",
      "Batch Number: 110001\t\t[110001/414113\t(26.563%)]\tLoss: 1387.735718\n",
      "Batch Number: 112001\t\t[112001/414113\t(27.046%)]\tLoss: 269.680298\n",
      "Batch Number: 114001\t\t[114001/414113\t(27.529%)]\tLoss: 119.254822\n",
      "Batch Number: 116001\t\t[116001/414113\t(28.012%)]\tLoss: 1074.715332\n",
      "Batch Number: 118001\t\t[118001/414113\t(28.495%)]\tLoss: 1329.216309\n",
      "Batch Number: 120001\t\t[120001/414113\t(28.978%)]\tLoss: 59.651489\n",
      "Batch Number: 122001\t\t[122001/414113\t(29.461%)]\tLoss: 502.728699\n",
      "Batch Number: 124001\t\t[124001/414113\t(29.944%)]\tLoss: 56.528320\n",
      "Batch Number: 126001\t\t[126001/414113\t(30.426%)]\tLoss: 92.301758\n",
      "Batch Number: 128001\t\t[128001/414113\t(30.909%)]\tLoss: 471.605469\n",
      "Batch Number: 130001\t\t[130001/414113\t(31.392%)]\tLoss: 4327.076172\n",
      "Batch Number: 132001\t\t[132001/414113\t(31.875%)]\tLoss: 15.419434\n",
      "Batch Number: 134001\t\t[134001/414113\t(32.358%)]\tLoss: 3732.269043\n",
      "Batch Number: 136001\t\t[136001/414113\t(32.841%)]\tLoss: 3482.992432\n",
      "Batch Number: 138001\t\t[138001/414113\t(33.324%)]\tLoss: 135.739868\n",
      "Batch Number: 140001\t\t[140001/414113\t(33.807%)]\tLoss: 113.262939\n",
      "Batch Number: 142001\t\t[142001/414113\t(34.290%)]\tLoss: 826.908447\n",
      "Batch Number: 144001\t\t[144001/414113\t(34.773%)]\tLoss: 321.084595\n",
      "Batch Number: 146001\t\t[146001/414113\t(35.256%)]\tLoss: 414.514893\n",
      "Batch Number: 148001\t\t[148001/414113\t(35.739%)]\tLoss: 802.187988\n",
      "Batch Number: 150001\t\t[150001/414113\t(36.222%)]\tLoss: 2927.254639\n",
      "Batch Number: 152001\t\t[152001/414113\t(36.705%)]\tLoss: 13971.939453\n",
      "Batch Number: 154001\t\t[154001/414113\t(37.188%)]\tLoss: 1992.723877\n",
      "Batch Number: 156001\t\t[156001/414113\t(37.671%)]\tLoss: 947.093506\n",
      "Batch Number: 158001\t\t[158001/414113\t(38.154%)]\tLoss: 89.710693\n",
      "Batch Number: 160001\t\t[160001/414113\t(38.637%)]\tLoss: 1433.969727\n",
      "Batch Number: 162001\t\t[162001/414113\t(39.120%)]\tLoss: 195.624634\n",
      "Batch Number: 164001\t\t[164001/414113\t(39.603%)]\tLoss: 1004.646606\n",
      "Batch Number: 166001\t\t[166001/414113\t(40.086%)]\tLoss: 315.404175\n",
      "Batch Number: 168001\t\t[168001/414113\t(40.569%)]\tLoss: 33.572266\n",
      "Batch Number: 170001\t\t[170001/414113\t(41.052%)]\tLoss: 1349.305176\n",
      "Batch Number: 172001\t\t[172001/414113\t(41.535%)]\tLoss: 11729.347656\n",
      "Batch Number: 174001\t\t[174001/414113\t(42.018%)]\tLoss: 3498.909668\n",
      "Batch Number: 176001\t\t[176001/414113\t(42.500%)]\tLoss: 58.125488\n",
      "Batch Number: 178001\t\t[178001/414113\t(42.983%)]\tLoss: 1405.335205\n",
      "Batch Number: 180001\t\t[180001/414113\t(43.466%)]\tLoss: 3013.723633\n",
      "Batch Number: 182001\t\t[182001/414113\t(43.949%)]\tLoss: 60.832794\n",
      "Batch Number: 184001\t\t[184001/414113\t(44.432%)]\tLoss: 5509.898926\n",
      "Batch Number: 186001\t\t[186001/414113\t(44.915%)]\tLoss: 18.445770\n",
      "Batch Number: 188001\t\t[188001/414113\t(45.398%)]\tLoss: 135.120972\n",
      "Batch Number: 190001\t\t[190001/414113\t(45.881%)]\tLoss: 1299.374390\n",
      "Batch Number: 192001\t\t[192001/414113\t(46.364%)]\tLoss: 34.013428\n",
      "Batch Number: 194001\t\t[194001/414113\t(46.847%)]\tLoss: 105.211304\n",
      "Batch Number: 196001\t\t[196001/414113\t(47.330%)]\tLoss: 155.512939\n",
      "Batch Number: 198001\t\t[198001/414113\t(47.813%)]\tLoss: 1971.753662\n",
      "Batch Number: 200001\t\t[200001/414113\t(48.296%)]\tLoss: 3843.392334\n",
      "Batch Number: 202001\t\t[202001/414113\t(48.779%)]\tLoss: 782.273438\n",
      "Batch Number: 204001\t\t[204001/414113\t(49.262%)]\tLoss: 44.717773\n",
      "Batch Number: 206001\t\t[206001/414113\t(49.745%)]\tLoss: 5320.552734\n",
      "Batch Number: 208001\t\t[208001/414113\t(50.228%)]\tLoss: 61.145996\n",
      "Batch Number: 210001\t\t[210001/414113\t(50.711%)]\tLoss: 1072.868896\n",
      "Batch Number: 212001\t\t[212001/414113\t(51.194%)]\tLoss: 84.510254\n",
      "Batch Number: 214001\t\t[214001/414113\t(51.677%)]\tLoss: 259.885010\n",
      "Batch Number: 216001\t\t[216001/414113\t(52.160%)]\tLoss: 14121.798828\n",
      "Batch Number: 218001\t\t[218001/414113\t(52.643%)]\tLoss: 69.621368\n",
      "Batch Number: 220001\t\t[220001/414113\t(53.126%)]\tLoss: 101.366455\n",
      "Batch Number: 222001\t\t[222001/414113\t(53.609%)]\tLoss: 14.959473\n",
      "Batch Number: 224001\t\t[224001/414113\t(54.092%)]\tLoss: 42.537598\n",
      "Batch Number: 226001\t\t[226001/414113\t(54.574%)]\tLoss: 65.292236\n",
      "Batch Number: 228001\t\t[228001/414113\t(55.057%)]\tLoss: 50.703003\n",
      "Batch Number: 230001\t\t[230001/414113\t(55.540%)]\tLoss: 2147.166992\n",
      "Batch Number: 232001\t\t[232001/414113\t(56.023%)]\tLoss: 60.462891\n",
      "Batch Number: 234001\t\t[234001/414113\t(56.506%)]\tLoss: 6197.371582\n",
      "Batch Number: 236001\t\t[236001/414113\t(56.989%)]\tLoss: 76.591125\n",
      "Batch Number: 238001\t\t[238001/414113\t(57.472%)]\tLoss: 123.842285\n",
      "Batch Number: 240001\t\t[240001/414113\t(57.955%)]\tLoss: 105.884689\n",
      "Batch Number: 242001\t\t[242001/414113\t(58.438%)]\tLoss: 217.699707\n",
      "Batch Number: 244001\t\t[244001/414113\t(58.921%)]\tLoss: 8188.218750\n",
      "Batch Number: 246001\t\t[246001/414113\t(59.404%)]\tLoss: 4924.057617\n",
      "Batch Number: 248001\t\t[248001/414113\t(59.887%)]\tLoss: 848.455078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 250001\t\t[250001/414113\t(60.370%)]\tLoss: 818.938477\n",
      "Batch Number: 252001\t\t[252001/414113\t(60.853%)]\tLoss: 89.022949\n",
      "Batch Number: 254001\t\t[254001/414113\t(61.336%)]\tLoss: 85.345215\n",
      "Batch Number: 256001\t\t[256001/414113\t(61.819%)]\tLoss: 33.755615\n",
      "Batch Number: 258001\t\t[258001/414113\t(62.302%)]\tLoss: 858.268066\n",
      "Batch Number: 260001\t\t[260001/414113\t(62.785%)]\tLoss: 7459.902832\n",
      "Batch Number: 262001\t\t[262001/414113\t(63.268%)]\tLoss: 81.730713\n",
      "Batch Number: 264001\t\t[264001/414113\t(63.751%)]\tLoss: 707.504395\n",
      "Batch Number: 266001\t\t[266001/414113\t(64.234%)]\tLoss: 15327.789062\n",
      "Batch Number: 268001\t\t[268001/414113\t(64.717%)]\tLoss: 36.357422\n",
      "Batch Number: 270001\t\t[270001/414113\t(65.200%)]\tLoss: 984.885254\n",
      "Batch Number: 272001\t\t[272001/414113\t(65.683%)]\tLoss: 259.373047\n",
      "Batch Number: 274001\t\t[274001/414113\t(66.166%)]\tLoss: 718.761963\n",
      "Batch Number: 276001\t\t[276001/414113\t(66.648%)]\tLoss: 47.919922\n",
      "Batch Number: 278001\t\t[278001/414113\t(67.131%)]\tLoss: 13434.386719\n",
      "Batch Number: 280001\t\t[280001/414113\t(67.614%)]\tLoss: 672.324219\n",
      "Batch Number: 282001\t\t[282001/414113\t(68.097%)]\tLoss: 95.846680\n",
      "Batch Number: 284001\t\t[284001/414113\t(68.580%)]\tLoss: 324.164062\n",
      "Batch Number: 286001\t\t[286001/414113\t(69.063%)]\tLoss: 3600.873779\n",
      "Batch Number: 288001\t\t[288001/414113\t(69.546%)]\tLoss: 238.196289\n",
      "Batch Number: 290001\t\t[290001/414113\t(70.029%)]\tLoss: 350.729492\n",
      "Batch Number: 292001\t\t[292001/414113\t(70.512%)]\tLoss: 654.421875\n",
      "Batch Number: 294001\t\t[294001/414113\t(70.995%)]\tLoss: 3085.211182\n",
      "Batch Number: 296001\t\t[296001/414113\t(71.478%)]\tLoss: 4254.506836\n",
      "Batch Number: 298001\t\t[298001/414113\t(71.961%)]\tLoss: 1111.094727\n",
      "Batch Number: 300001\t\t[300001/414113\t(72.444%)]\tLoss: 117.996582\n",
      "Batch Number: 302001\t\t[302001/414113\t(72.927%)]\tLoss: 115.283203\n",
      "Batch Number: 304001\t\t[304001/414113\t(73.410%)]\tLoss: 1252.361328\n",
      "Batch Number: 306001\t\t[306001/414113\t(73.893%)]\tLoss: 9159.519531\n",
      "Batch Number: 308001\t\t[308001/414113\t(74.376%)]\tLoss: 213.837891\n",
      "Batch Number: 310001\t\t[310001/414113\t(74.859%)]\tLoss: 26.754395\n",
      "Batch Number: 312001\t\t[312001/414113\t(75.342%)]\tLoss: 262.619507\n",
      "Batch Number: 314001\t\t[314001/414113\t(75.825%)]\tLoss: 651.259033\n",
      "Batch Number: 316001\t\t[316001/414113\t(76.308%)]\tLoss: 1464.494141\n",
      "Batch Number: 318001\t\t[318001/414113\t(76.791%)]\tLoss: 1519.118164\n",
      "Batch Number: 320001\t\t[320001/414113\t(77.274%)]\tLoss: 21.357422\n",
      "Batch Number: 322001\t\t[322001/414113\t(77.757%)]\tLoss: 627.941895\n",
      "Batch Number: 324001\t\t[324001/414113\t(78.240%)]\tLoss: 1731.838867\n",
      "Batch Number: 326001\t\t[326001/414113\t(78.722%)]\tLoss: 568.285400\n",
      "Batch Number: 328001\t\t[328001/414113\t(79.205%)]\tLoss: 214.768555\n",
      "Batch Number: 330001\t\t[330001/414113\t(79.688%)]\tLoss: 288.676758\n",
      "Batch Number: 332001\t\t[332001/414113\t(80.171%)]\tLoss: 677.226562\n",
      "Batch Number: 334001\t\t[334001/414113\t(80.654%)]\tLoss: 331.210449\n",
      "Batch Number: 336001\t\t[336001/414113\t(81.137%)]\tLoss: 3191.426758\n",
      "Batch Number: 338001\t\t[338001/414113\t(81.620%)]\tLoss: 1709.244141\n",
      "Batch Number: 340001\t\t[340001/414113\t(82.103%)]\tLoss: 303.139160\n",
      "Batch Number: 342001\t\t[342001/414113\t(82.586%)]\tLoss: 1525.897583\n",
      "Batch Number: 344001\t\t[344001/414113\t(83.069%)]\tLoss: 832.965149\n",
      "Batch Number: 346001\t\t[346001/414113\t(83.552%)]\tLoss: 38.052734\n",
      "Batch Number: 348001\t\t[348001/414113\t(84.035%)]\tLoss: 40.445312\n",
      "Batch Number: 350001\t\t[350001/414113\t(84.518%)]\tLoss: 14860.275391\n",
      "Batch Number: 352001\t\t[352001/414113\t(85.001%)]\tLoss: 26614.548828\n",
      "Batch Number: 354001\t\t[354001/414113\t(85.484%)]\tLoss: 71.989258\n",
      "Batch Number: 356001\t\t[356001/414113\t(85.967%)]\tLoss: 3732.023193\n",
      "Batch Number: 358001\t\t[358001/414113\t(86.450%)]\tLoss: 26.403809\n",
      "Batch Number: 360001\t\t[360001/414113\t(86.933%)]\tLoss: 5913.478516\n",
      "Batch Number: 362001\t\t[362001/414113\t(87.416%)]\tLoss: 1721.786621\n",
      "Batch Number: 364001\t\t[364001/414113\t(87.899%)]\tLoss: 261.296387\n",
      "Batch Number: 366001\t\t[366001/414113\t(88.382%)]\tLoss: 3164.315430\n",
      "Batch Number: 368001\t\t[368001/414113\t(88.865%)]\tLoss: 336.604980\n",
      "Batch Number: 370001\t\t[370001/414113\t(89.348%)]\tLoss: 1283.243652\n",
      "Batch Number: 372001\t\t[372001/414113\t(89.831%)]\tLoss: 1577.368652\n",
      "Batch Number: 374001\t\t[374001/414113\t(90.314%)]\tLoss: 33.758789\n",
      "Batch Number: 376001\t\t[376001/414113\t(90.796%)]\tLoss: 100.619141\n",
      "Batch Number: 378001\t\t[378001/414113\t(91.279%)]\tLoss: 48.791626\n",
      "Batch Number: 380001\t\t[380001/414113\t(91.762%)]\tLoss: 950.405273\n",
      "Batch Number: 382001\t\t[382001/414113\t(92.245%)]\tLoss: 1182.810425\n",
      "Batch Number: 384001\t\t[384001/414113\t(92.728%)]\tLoss: 122.492188\n",
      "Batch Number: 386001\t\t[386001/414113\t(93.211%)]\tLoss: 373.316406\n",
      "Batch Number: 388001\t\t[388001/414113\t(93.694%)]\tLoss: 35.104492\n",
      "Batch Number: 390001\t\t[390001/414113\t(94.177%)]\tLoss: 7025.182617\n",
      "Batch Number: 392001\t\t[392001/414113\t(94.660%)]\tLoss: 258.362305\n",
      "Batch Number: 394001\t\t[394001/414113\t(95.143%)]\tLoss: 745.740234\n",
      "Batch Number: 396001\t\t[396001/414113\t(95.626%)]\tLoss: 1608.523682\n",
      "Batch Number: 398001\t\t[398001/414113\t(96.109%)]\tLoss: 797.240234\n",
      "Batch Number: 400001\t\t[400001/414113\t(96.592%)]\tLoss: 12159.268555\n",
      "Batch Number: 402001\t\t[402001/414113\t(97.075%)]\tLoss: 5668.770020\n",
      "Batch Number: 404001\t\t[404001/414113\t(97.558%)]\tLoss: 71.640137\n",
      "Batch Number: 406001\t\t[406001/414113\t(98.041%)]\tLoss: 18695.318359\n",
      "Batch Number: 408001\t\t[408001/414113\t(98.524%)]\tLoss: 3352.027344\n",
      "Batch Number: 410001\t\t[410001/414113\t(99.007%)]\tLoss: 535.497559\n",
      "Batch Number: 412001\t\t[412001/414113\t(99.490%)]\tLoss: 8296.035156\n",
      "Batch Number: 414001\t\t[414001/414113\t(99.973%)]\tLoss: 37.400146\n",
      "Train Epoch: 1 [414113/414113 (100%)]\tLoss: 14993.779297\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d69540d23226>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(max_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     train_losses, val_losses, val_accs = train(max_epochs, encoder, decoder, train_loader, val_loader, \n\u001b[0;32m---> 39\u001b[0;31m                                                optimizer, loss_fn, vocab_size, verbose=True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mbest_model_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b2199746e537>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_epochs, encoder, model, train_loader, val_loader, optimizer, loss_fn, vocab_size, verbose, print_every)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m#target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6071\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training samples:  414113\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation samples:  202654\n",
      "Batch Number: 1\t\t[1/202654\t(0.000%)]\tLoss: 19.784124\n",
      "Batch Number: 2001\t\t[2001/202654\t(0.987%)]\tLoss: 33.342056\n",
      "Batch Number: 4001\t\t[4001/202654\t(1.974%)]\tLoss: 29.810665\n",
      "Batch Number: 6001\t\t[6001/202654\t(2.961%)]\tLoss: 26.401575\n",
      "Batch Number: 8001\t\t[8001/202654\t(3.948%)]\tLoss: 33.347885\n",
      "Batch Number: 10001\t\t[10001/202654\t(4.935%)]\tLoss: 23.015902\n",
      "Batch Number: 12001\t\t[12001/202654\t(5.921%)]\tLoss: 23.010170\n",
      "Batch Number: 14001\t\t[14001/202654\t(6.908%)]\tLoss: 36.964104\n",
      "Batch Number: 16001\t\t[16001/202654\t(7.895%)]\tLoss: 26.381308\n",
      "Batch Number: 18001\t\t[18001/202654\t(8.882%)]\tLoss: 23.019016\n",
      "Batch Number: 20001\t\t[20001/202654\t(9.869%)]\tLoss: 40.644882\n",
      "Batch Number: 22001\t\t[22001/202654\t(10.856%)]\tLoss: 33.355251\n",
      "Batch Number: 24001\t\t[24001/202654\t(11.843%)]\tLoss: 23.035511\n",
      "Batch Number: 26001\t\t[26001/202654\t(12.830%)]\tLoss: 26.371870\n",
      "Batch Number: 28001\t\t[28001/202654\t(13.817%)]\tLoss: 23.039320\n",
      "Batch Number: 30001\t\t[30001/202654\t(14.804%)]\tLoss: 36.960453\n",
      "Batch Number: 32001\t\t[32001/202654\t(15.790%)]\tLoss: 26.415747\n",
      "Batch Number: 34001\t\t[34001/202654\t(16.777%)]\tLoss: 23.011129\n",
      "Batch Number: 36001\t\t[36001/202654\t(17.764%)]\tLoss: 26.393436\n",
      "Batch Number: 38001\t\t[38001/202654\t(18.751%)]\tLoss: 29.827038\n",
      "Batch Number: 40001\t\t[40001/202654\t(19.738%)]\tLoss: 40.609711\n",
      "Batch Number: 42001\t\t[42001/202654\t(20.725%)]\tLoss: 26.380478\n",
      "Batch Number: 44001\t\t[44001/202654\t(21.712%)]\tLoss: 29.801016\n",
      "Batch Number: 46001\t\t[46001/202654\t(22.699%)]\tLoss: 23.043205\n",
      "Batch Number: 48001\t\t[48001/202654\t(23.686%)]\tLoss: 36.958500\n",
      "Batch Number: 50001\t\t[50001/202654\t(24.673%)]\tLoss: 29.817265\n",
      "Batch Number: 52001\t\t[52001/202654\t(25.659%)]\tLoss: 26.383230\n",
      "Batch Number: 54001\t\t[54001/202654\t(26.646%)]\tLoss: 29.811007\n",
      "Batch Number: 56001\t\t[56001/202654\t(27.633%)]\tLoss: 26.430073\n",
      "Batch Number: 58001\t\t[58001/202654\t(28.620%)]\tLoss: 29.828493\n",
      "Batch Number: 60001\t\t[60001/202654\t(29.607%)]\tLoss: 16.628399\n",
      "Batch Number: 62001\t\t[62001/202654\t(30.594%)]\tLoss: 33.333721\n",
      "Batch Number: 64001\t\t[64001/202654\t(31.581%)]\tLoss: 33.335667\n",
      "Batch Number: 66001\t\t[66001/202654\t(32.568%)]\tLoss: 23.031069\n",
      "Batch Number: 68001\t\t[68001/202654\t(33.555%)]\tLoss: 19.775444\n",
      "Batch Number: 70001\t\t[70001/202654\t(34.542%)]\tLoss: 29.838020\n",
      "Batch Number: 72001\t\t[72001/202654\t(35.529%)]\tLoss: 26.383486\n",
      "Batch Number: 74001\t\t[74001/202654\t(36.515%)]\tLoss: 26.369644\n",
      "Batch Number: 76001\t\t[76001/202654\t(37.502%)]\tLoss: 23.019333\n",
      "Batch Number: 78001\t\t[78001/202654\t(38.489%)]\tLoss: 19.782503\n",
      "Batch Number: 80001\t\t[80001/202654\t(39.476%)]\tLoss: 26.392756\n",
      "Batch Number: 82001\t\t[82001/202654\t(40.463%)]\tLoss: 29.836849\n",
      "Batch Number: 84001\t\t[84001/202654\t(41.450%)]\tLoss: 26.390961\n",
      "Batch Number: 86001\t\t[86001/202654\t(42.437%)]\tLoss: 26.391436\n",
      "Batch Number: 88001\t\t[88001/202654\t(43.424%)]\tLoss: 26.369991\n",
      "Batch Number: 90001\t\t[90001/202654\t(44.411%)]\tLoss: 36.958282\n",
      "Batch Number: 92001\t\t[92001/202654\t(45.398%)]\tLoss: 26.347988\n",
      "Batch Number: 94001\t\t[94001/202654\t(46.384%)]\tLoss: 40.655197\n",
      "Batch Number: 96001\t\t[96001/202654\t(47.371%)]\tLoss: 33.323578\n",
      "Batch Number: 98001\t\t[98001/202654\t(48.358%)]\tLoss: 36.949375\n",
      "Batch Number: 100001\t\t[100001/202654\t(49.345%)]\tLoss: 40.610466\n",
      "Batch Number: 102001\t\t[102001/202654\t(50.332%)]\tLoss: 23.029400\n",
      "Batch Number: 104001\t\t[104001/202654\t(51.319%)]\tLoss: 23.030140\n",
      "Batch Number: 106001\t\t[106001/202654\t(52.306%)]\tLoss: 33.339600\n",
      "Batch Number: 108001\t\t[108001/202654\t(53.293%)]\tLoss: 19.769012\n",
      "Batch Number: 110001\t\t[110001/202654\t(54.280%)]\tLoss: 33.380783\n",
      "Batch Number: 112001\t\t[112001/202654\t(55.267%)]\tLoss: 23.027771\n",
      "Batch Number: 114001\t\t[114001/202654\t(56.254%)]\tLoss: 29.805897\n",
      "Batch Number: 116001\t\t[116001/202654\t(57.240%)]\tLoss: 19.758600\n",
      "Batch Number: 118001\t\t[118001/202654\t(58.227%)]\tLoss: 26.404240\n",
      "Batch Number: 120001\t\t[120001/202654\t(59.214%)]\tLoss: 16.623644\n",
      "Batch Number: 122001\t\t[122001/202654\t(60.201%)]\tLoss: 23.013174\n",
      "Batch Number: 124001\t\t[124001/202654\t(61.188%)]\tLoss: 23.038084\n",
      "Batch Number: 126001\t\t[126001/202654\t(62.175%)]\tLoss: 23.031767\n",
      "Batch Number: 128001\t\t[128001/202654\t(63.162%)]\tLoss: 23.022541\n",
      "Batch Number: 130001\t\t[130001/202654\t(64.149%)]\tLoss: 29.797997\n",
      "Batch Number: 132001\t\t[132001/202654\t(65.136%)]\tLoss: 23.025827\n",
      "Batch Number: 134001\t\t[134001/202654\t(66.123%)]\tLoss: 26.380199\n",
      "Batch Number: 136001\t\t[136001/202654\t(67.109%)]\tLoss: 40.627106\n",
      "Batch Number: 138001\t\t[138001/202654\t(68.096%)]\tLoss: 29.855314\n",
      "Batch Number: 140001\t\t[140001/202654\t(69.083%)]\tLoss: 36.952499\n",
      "Batch Number: 142001\t\t[142001/202654\t(70.070%)]\tLoss: 23.023380\n",
      "Batch Number: 144001\t\t[144001/202654\t(71.057%)]\tLoss: 26.378115\n",
      "Batch Number: 146001\t\t[146001/202654\t(72.044%)]\tLoss: 29.809820\n",
      "Batch Number: 148001\t\t[148001/202654\t(73.031%)]\tLoss: 29.828558\n",
      "Batch Number: 150001\t\t[150001/202654\t(74.018%)]\tLoss: 29.807455\n",
      "Batch Number: 152001\t\t[152001/202654\t(75.005%)]\tLoss: 23.041658\n",
      "Batch Number: 154001\t\t[154001/202654\t(75.992%)]\tLoss: 33.331650\n",
      "Batch Number: 156001\t\t[156001/202654\t(76.978%)]\tLoss: 40.601955\n",
      "Batch Number: 158001\t\t[158001/202654\t(77.965%)]\tLoss: 26.373077\n",
      "Batch Number: 160001\t\t[160001/202654\t(78.952%)]\tLoss: 40.608215\n",
      "Batch Number: 162001\t\t[162001/202654\t(79.939%)]\tLoss: 33.346447\n",
      "Batch Number: 164001\t\t[164001/202654\t(80.926%)]\tLoss: 23.032381\n",
      "Batch Number: 166001\t\t[166001/202654\t(81.913%)]\tLoss: 44.326050\n",
      "Batch Number: 168001\t\t[168001/202654\t(82.900%)]\tLoss: 23.038286\n",
      "Batch Number: 170001\t\t[170001/202654\t(83.887%)]\tLoss: 33.343002\n",
      "Batch Number: 172001\t\t[172001/202654\t(84.874%)]\tLoss: 26.371147\n",
      "Batch Number: 174001\t\t[174001/202654\t(85.861%)]\tLoss: 29.871109\n",
      "Batch Number: 176001\t\t[176001/202654\t(86.848%)]\tLoss: 26.367416\n",
      "Batch Number: 178001\t\t[178001/202654\t(87.834%)]\tLoss: 36.939751\n",
      "Batch Number: 180001\t\t[180001/202654\t(88.821%)]\tLoss: 26.345020\n",
      "Batch Number: 182001\t\t[182001/202654\t(89.808%)]\tLoss: 33.340103\n",
      "Batch Number: 184001\t\t[184001/202654\t(90.795%)]\tLoss: 23.026352\n",
      "Batch Number: 186001\t\t[186001/202654\t(91.782%)]\tLoss: 29.816549\n",
      "Batch Number: 188001\t\t[188001/202654\t(92.769%)]\tLoss: 26.374645\n",
      "Batch Number: 190001\t\t[190001/202654\t(93.756%)]\tLoss: 44.386581\n",
      "Batch Number: 192001\t\t[192001/202654\t(94.743%)]\tLoss: 29.790104\n",
      "Batch Number: 194001\t\t[194001/202654\t(95.730%)]\tLoss: 29.815331\n",
      "Batch Number: 196001\t\t[196001/202654\t(96.717%)]\tLoss: 26.382179\n",
      "Batch Number: 198001\t\t[198001/202654\t(97.703%)]\tLoss: 33.370602\n",
      "Batch Number: 200001\t\t[200001/202654\t(98.690%)]\tLoss: 26.370668\n",
      "Batch Number: 202001\t\t[202001/202654\t(99.677%)]\tLoss: 33.348896\n",
      "Batch Number: 204001\t\t[204001/202654\t(100.664%)]\tLoss: 29.830008\n",
      "Batch Number: 206001\t\t[206001/202654\t(101.651%)]\tLoss: 33.324997\n",
      "Batch Number: 208001\t\t[208001/202654\t(102.638%)]\tLoss: 29.825134\n",
      "Batch Number: 210001\t\t[210001/202654\t(103.625%)]\tLoss: 23.039074\n",
      "Batch Number: 212001\t\t[212001/202654\t(104.612%)]\tLoss: 23.004919\n",
      "Batch Number: 214001\t\t[214001/202654\t(105.599%)]\tLoss: 29.832273\n",
      "Batch Number: 216001\t\t[216001/202654\t(106.586%)]\tLoss: 26.413851\n",
      "Batch Number: 218001\t\t[218001/202654\t(107.573%)]\tLoss: 29.788361\n",
      "Batch Number: 220001\t\t[220001/202654\t(108.559%)]\tLoss: 36.950462\n",
      "Batch Number: 222001\t\t[222001/202654\t(109.546%)]\tLoss: 23.056078\n",
      "Batch Number: 224001\t\t[224001/202654\t(110.533%)]\tLoss: 23.019669\n",
      "Batch Number: 226001\t\t[226001/202654\t(111.520%)]\tLoss: 29.848072\n",
      "Batch Number: 228001\t\t[228001/202654\t(112.507%)]\tLoss: 33.345062\n",
      "Batch Number: 230001\t\t[230001/202654\t(113.494%)]\tLoss: 23.048279\n",
      "Batch Number: 232001\t\t[232001/202654\t(114.481%)]\tLoss: 23.029272\n",
      "Batch Number: 234001\t\t[234001/202654\t(115.468%)]\tLoss: 29.810947\n",
      "Batch Number: 236001\t\t[236001/202654\t(116.455%)]\tLoss: 23.021185\n",
      "Batch Number: 238001\t\t[238001/202654\t(117.442%)]\tLoss: 33.369743\n",
      "Batch Number: 240001\t\t[240001/202654\t(118.428%)]\tLoss: 26.397999\n",
      "Batch Number: 242001\t\t[242001/202654\t(119.415%)]\tLoss: 26.395426\n",
      "Batch Number: 244001\t\t[244001/202654\t(120.402%)]\tLoss: 26.380259\n",
      "Batch Number: 246001\t\t[246001/202654\t(121.389%)]\tLoss: 29.807442\n",
      "Batch Number: 248001\t\t[248001/202654\t(122.376%)]\tLoss: 23.013824\n",
      "Batch Number: 250001\t\t[250001/202654\t(123.363%)]\tLoss: 36.940315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 252001\t\t[252001/202654\t(124.350%)]\tLoss: 29.800743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d69540d23226>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(max_epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     train_losses, val_losses, val_accs = train(max_epochs, encoder, decoder, train_loader, val_loader, \n\u001b[0;32m---> 39\u001b[0;31m                                                optimizer, loss_fn, vocab_size, verbose=True)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mbest_model_wts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-987f6a9d64f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_epochs, encoder, model, train_loader, val_loader, optimizer, loss_fn, vocab_size, verbose, print_every)\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
