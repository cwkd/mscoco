{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from model import Encoder, Decoder\n",
    "from dataloader2 import Vocabulary, MSCOCODataset, one_hot_encode, un_one_hot_encode\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epochs=10, encoder= None, model=None, train_loader=None, val_loader=None, \n",
    "          optimizer=None, loss_fn=None, vocab_size=None, verbose=False, print_every=2000):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    batch_size = train_loader.batch_size\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            gt = target.copy()\n",
    "            target = torch.cat(target[:-1]).unsqueeze(0)\n",
    "            gt = torch.cat(gt[1:]).unsqueeze(0)\n",
    "            data, target, gt = data.to(device), target.to(device), gt.to(device)\n",
    "            features = encoder(data).unsqueeze(0)\n",
    "            features = features.repeat([1, gt.shape[1], 1])\n",
    "            #print(features.shape)\n",
    "            output, (hidden, cs) = model(features, target)\n",
    "            output = softmax(output)\n",
    "            #optimizer.zero_grad()\n",
    "            #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "            #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "            #print(data.shape, target.shape)\n",
    "            loss = 0\n",
    "            for b in range(batch_size):\n",
    "                for e in range(len(gt[b])):\n",
    "                    l = loss_fn(output[b,e].unsqueeze(0), gt[b,e].long().unsqueeze(0))\n",
    "                    loss += l\n",
    "            loss/batch_size\n",
    "            #pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "            #print(pred.shape, target.shape)\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            if verbose and batch_idx*batch_size % print_every < batch_size:\n",
    "                print('Batch Number: {}\\t\\t[{}/{}\\t({:.3f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        batch_idx + 1, (batch_idx +1)* train_loader.batch_size, len(train_loader),\n",
    "                        100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if batch_idx*batch_size % 5000 < batch_size and batch_idx!=0:\n",
    "            #    sample(model, temp)\n",
    "            \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch + 1, (batch_idx+1) * train_loader.batch_size, len(train_loader),\n",
    "            100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output, hidden, cs = model(data)\n",
    "                #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "                #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "                loss = 0\n",
    "                for b in range(batch_size):\n",
    "                    for e in range(len(target[b])):\n",
    "                        l = loss_fn(output[b,e].unsqueeze(0), target[b,e].long().unsqueeze(0))\n",
    "                        loss += l\n",
    "                        total += 1\n",
    "                batch_loss = loss.item()\n",
    "                val_loss += batch_loss\n",
    "                pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "                #print(pred, target)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            #if val_loss < best_loss[0]:\n",
    "            #    print('\\nSaving model parameters in epoch {}'.format(epoch+1))\n",
    "            #    torch.save(model.state_dict(), \"./temp_model.pt\")\n",
    "            #    best_loss[0] = val_loss\n",
    "            #    save_flag = True\n",
    "\n",
    "            val_acc = 100. * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            print('Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                val_loss, correct, total,\n",
    "                val_acc))\n",
    "            \n",
    "        #sample(model, temp)\n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_epochs=1, batch_size=1, lr=0.01):\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    vocab_size = len(vocab)\n",
    "    #print(vocab.word2idx['<end>'])\n",
    "    \n",
    "    print('Vocab Size: {}'.format(vocab_size))\n",
    "    transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),])\n",
    "    \n",
    "    \n",
    "    trainset = MSCOCODataset(root = './train2014/train2014/',\n",
    "                            annFile = './annotations_trainval2014/annotations/captions_train2014.json',\n",
    "                            transform=transform, vocab=vocab)\n",
    "    print('Number of training samples: ', len(trainset))\n",
    "    \n",
    "    valset = MSCOCODataset(root = './val2014/',\n",
    "                            annFile = './annotations_trainval2014/annotations/captions_val2014.json',\n",
    "                            transform=transform, vocab=vocab)\n",
    "\n",
    "    \n",
    "    print('Number of validation samples: ', len(valset))\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    encoder = Encoder().to(device)\n",
    "    decoder = Decoder(feat_size=4096, embed_size=1000, \n",
    "                      vocab_size=vocab_size, hidden_size=300, \n",
    "                      num_layers=3).to(device)\n",
    "    \n",
    "    train_loader = data.DataLoader(trainset, batch_size, shuffle=False)\n",
    "    val_loader = data.DataLoader(valset, batch_size, shuffle=False)\n",
    "    optimizer = optim.RMSprop(decoder.parameters(), lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    train_losses, val_losses, val_accs = train(max_epochs, encoder, decoder, train_loader, val_loader, \n",
    "                                               optimizer, loss_fn, vocab_size, verbose=True)\n",
    "    \n",
    "    best_model_wts = decoder.state_dict()\n",
    "    torch.save(best_model_wts,'./model.pt')\n",
    "\n",
    "    epochs = np.arange(1, max_epochs+1)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(epochs, train_losses, epochs, val_losses)\n",
    "    plt.legend(['Training', 'Validation'],loc='upper right')\n",
    "    plt.title('Training and Validation Loss per epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(epochs, val_accs)\n",
    "    plt.legend(['Validation'],loc='upper right')\n",
    "    plt.title('Validation Accuracy per epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6071\n",
      "loading annotations into memory...\n",
      "Done (t=1.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training samples:  414113\n",
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation samples:  202654\n",
      "Batch Number: 1\t\t[1/414113\t(0.000%)]\tLoss: 19.767372\n",
      "Batch Number: 2001\t\t[2001/414113\t(0.483%)]\tLoss: 113.727623\n",
      "Batch Number: 4001\t\t[4001/414113\t(0.966%)]\tLoss: 60.076023\n",
      "Batch Number: 6001\t\t[6001/414113\t(1.449%)]\tLoss: 58.060211\n",
      "Batch Number: 8001\t\t[8001/414113\t(1.932%)]\tLoss: 31.822937\n",
      "Batch Number: 10001\t\t[10001/414113\t(2.415%)]\tLoss: 39.057373\n",
      "Batch Number: 12001\t\t[12001/414113\t(2.898%)]\tLoss: 92.366089\n",
      "Batch Number: 14001\t\t[14001/414113\t(3.381%)]\tLoss: 107.205063\n",
      "Batch Number: 16001\t\t[16001/414113\t(3.864%)]\tLoss: 271.430054\n",
      "Batch Number: 18001\t\t[18001/414113\t(4.347%)]\tLoss: 44.846802\n",
      "Batch Number: 20001\t\t[20001/414113\t(4.830%)]\tLoss: 47.456726\n",
      "Batch Number: 22001\t\t[22001/414113\t(5.313%)]\tLoss: 155.898422\n",
      "Batch Number: 24001\t\t[24001/414113\t(5.796%)]\tLoss: 5543.311523\n",
      "Batch Number: 26001\t\t[26001/414113\t(6.278%)]\tLoss: 545.223877\n",
      "Batch Number: 28001\t\t[28001/414113\t(6.761%)]\tLoss: 31.284363\n",
      "Batch Number: 30001\t\t[30001/414113\t(7.244%)]\tLoss: 200.868286\n",
      "Batch Number: 32001\t\t[32001/414113\t(7.727%)]\tLoss: 47.332581\n",
      "Batch Number: 34001\t\t[34001/414113\t(8.210%)]\tLoss: 647.789062\n",
      "Batch Number: 36001\t\t[36001/414113\t(8.693%)]\tLoss: 254.915283\n",
      "Batch Number: 38001\t\t[38001/414113\t(9.176%)]\tLoss: 29.160217\n",
      "Batch Number: 40001\t\t[40001/414113\t(9.659%)]\tLoss: 211.652222\n",
      "Batch Number: 42001\t\t[42001/414113\t(10.142%)]\tLoss: 992.191040\n",
      "Batch Number: 44001\t\t[44001/414113\t(10.625%)]\tLoss: 645.768066\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
