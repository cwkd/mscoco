{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from model import Encoder, Decoder\n",
    "from dataloader2 import Vocabulary, MSCOCODataset, one_hot_encode, un_one_hot_encode\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_epochs=10, encoder= None, model=None, train_loader=None, val_loader=None, \n",
    "          optimizer=None, loss_fn=None, vocab_size=None, verbose=False, print_every=2000):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    batch_size = train_loader.batch_size\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    loss = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            gt = target.copy()\n",
    "            target = torch.cat(target[:-1]).unsqueeze(0)\n",
    "            gt = torch.cat(gt[1:]).unsqueeze(0)\n",
    "            data, target, gt = data.to(device), target.to(device), gt.to(device)\n",
    "            features = encoder(data).unsqueeze(0)\n",
    "            features = features.repeat([1, gt.shape[1], 1])\n",
    "            #print(features.shape)\n",
    "            output, (hidden, cs) = model(features, target)\n",
    "            output = softmax(output)\n",
    "            #optimizer.zero_grad()\n",
    "            #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "            #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "            #print(data.shape, target.shape)\n",
    "            loss = 0\n",
    "            for b in range(batch_size):\n",
    "                for e in range(len(gt[b])):\n",
    "                    l = loss_fn(output[b,e].unsqueeze(0), gt[b,e].long().unsqueeze(0))\n",
    "                    loss += l\n",
    "            loss/batch_size\n",
    "            #pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "            #print(pred.shape, target.shape)\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            if verbose and batch_idx*batch_size % print_every < batch_size:\n",
    "                print('Batch Number: {}\\t\\t[{}/{}\\t({:.3f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        batch_idx + 1, (batch_idx +1)* train_loader.batch_size, len(train_loader),\n",
    "                        100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if batch_idx*batch_size % 5000 < batch_size and batch_idx!=0:\n",
    "            #    sample(model, temp)\n",
    "            \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch + 1, (batch_idx+1) * train_loader.batch_size, len(train_loader),\n",
    "            100. * batch_idx / (len(train_loader)/train_loader.batch_size), loss.item()))\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, target in val_loader:\n",
    "                gt = target.copy()\n",
    "                target = torch.cat(target[:-1]).unsqueeze(0)\n",
    "                gt = torch.cat(gt[1:]).unsqueeze(0)\n",
    "                data, target, gt = data.to(device), target.to(device), gt.to(device)\n",
    "                features = encoder(data).unsqueeze(0)\n",
    "                features = features.repeat([1, gt.shape[1], 1])\n",
    "                output, (hidden, cs) = model(features, target)\n",
    "                #target, _ = nn.utils.rnn.pad_packed_sequence(target, batch_first=True)\n",
    "                #data, _ = nn.utils.rnn.pad_packed_sequence(data, batch_first=True)\n",
    "                loss = 0\n",
    "                for b in range(batch_size):\n",
    "                    for e in range(len(gt[b])):\n",
    "                        l = loss_fn(output[b,e].unsqueeze(0), gt[b,e].long().unsqueeze(0))\n",
    "                        loss += l\n",
    "                        total += 1\n",
    "                batch_loss = loss.item()\n",
    "                val_loss += batch_loss\n",
    "                pred = output.argmax(dim=2, keepdim=True) # get the index of the max log-probability\n",
    "                #print(pred, target)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            #if val_loss < best_loss[0]:\n",
    "            #    print('\\nSaving model parameters in epoch {}'.format(epoch+1))\n",
    "            #    torch.save(model.state_dict(), \"./temp_model.pt\")\n",
    "            #    best_loss[0] = val_loss\n",
    "            #    save_flag = True\n",
    "\n",
    "            val_acc = 100. * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            print('Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                val_loss, correct, total,\n",
    "                val_acc))\n",
    "            \n",
    "        #sample(model, temp)\n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(max_epochs=1, batch_size=1, lr=1e-4):\n",
    "    with open('vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    vocab_size = len(vocab)\n",
    "    #print(vocab.word2idx['<end>'])\n",
    "    \n",
    "    print('Vocab Size: {}'.format(vocab_size))\n",
    "    transform = transforms.Compose([transforms.Resize(224),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),])\n",
    "    \n",
    "    \n",
    "    trainset = MSCOCODataset(root = './train2014/train2014/',\n",
    "                            annFile = './annotations_trainval2014/annotations/captions_train2014.json',\n",
    "                            transform=transform, vocab=vocab)\n",
    "    print('Number of training samples: ', len(trainset))\n",
    "    \n",
    "    valset = MSCOCODataset(root = './val2014/',\n",
    "                            annFile = './annotations_trainval2014/annotations/captions_val2014.json',\n",
    "                            transform=transform, vocab=vocab)\n",
    "\n",
    "    \n",
    "    print('Number of validation samples: ', len(valset))\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    encoder = Encoder().to(device)\n",
    "    decoder = Decoder(feat_size=4096, embed_size=1000, \n",
    "                      vocab_size=vocab_size, hidden_size=300, \n",
    "                      num_layers=3).to(device)\n",
    "    \n",
    "    train_loader = data.DataLoader(trainset, batch_size, shuffle=False)\n",
    "    val_loader = data.DataLoader(valset, batch_size, shuffle=False)\n",
    "    optimizer = optim.RMSprop(decoder.parameters(), lr)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    train_losses, val_losses, val_accs = train(max_epochs, encoder, decoder, train_loader, val_loader, \n",
    "                                               optimizer, loss_fn, vocab_size, verbose=True)\n",
    "    \n",
    "    best_model_wts = decoder.state_dict()\n",
    "    torch.save(best_model_wts,'./model.pt')\n",
    "\n",
    "    epochs = np.arange(1, max_epochs+1)\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(epochs, train_losses, epochs, val_losses)\n",
    "    plt.legend(['Training', 'Validation'],loc='upper right')\n",
    "    plt.title('Training and Validation Loss per epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xticks(epochs)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.plot(epochs, val_accs)\n",
    "    plt.legend(['Validation'],loc='upper right')\n",
    "    plt.title('Validation Accuracy per epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    train_losses = np.asarray(train_losses)\n",
    "    val_losses = np.asarray(val_losses)\n",
    "    val_accs = np.asarray(val_accs)\n",
    "    np.save('./train_losses.npy',train_losses)\n",
    "    np.save('./val_losses.npy',train_losses)\n",
    "    np.save('./val_accs.npy',train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6071\n",
      "loading annotations into memory...\n",
      "Done (t=1.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training samples:  414113\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of validation samples:  202654\n",
      "Batch Number: 1\t\t[1/414113\t(0.000%)]\tLoss: 19.767372\n",
      "Train Epoch: 1 [1/414113 (0%)]\tLoss: 19.767372\n",
      "Val set: Average loss: 0.0000, Accuracy: 0/12 (0.00%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Daniel\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
